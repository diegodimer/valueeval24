# -*- coding: utf-8 -*-
"""Only Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1peMNQqOMjGBSp_2GxI0VXSuJCn9JhA-J

# Fine-tuning BERT and RoBERTa for multi-label text classification

## Setup environment
"""
import ast
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, EvalPrediction, set_seed
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, hamming_loss, recall_score
import torch

np.random.seed(42)
torch.manual_seed(42)
set_seed(42)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using {device}")
training_loop_stream = torch.cuda.Stream()
torch.cuda.synchronize() # make sure model is on device
with torch.cuda.stream(training_loop_stream):
    TARGET_LIST = ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance']

    df_data = pd.read_csv("df_data.csv")
    df_data = df_data.drop_duplicates(['text'])
    df_data = df_data[df_data['text'].str.len() > 10]
    df_data.reset_index(drop=True, inplace=True)
    df_val = pd.read_csv("df_val.csv")
    df_data['label'] = df_data['label'].apply(lambda x: ast.literal_eval(x))
    df_val['label'] = df_val['label'].apply(lambda x: ast.literal_eval(x))
    df_val = df_val.drop_duplicates(['text'])
    df_val = df_val[df_val['text'].str.len() > 10]
    df_val.reset_index(drop=True, inplace=True)

    labels = [label for label in TARGET_LIST if label not in ["Text-ID", "Sentence-ID"]]
    id2label = {idx:label for idx, label in enumerate(labels)}
    label2id = {label:idx for idx, label in enumerate(labels)}

    """## CustomDataset
    We'll use this custom dataset class to apply the tokenizer to our pandas dataframe.
    It is important to notice that 'labels' is in the return for the getitem function, as this is how the model will calculate the loss
    """
    import sys
    MODEL_NAME = sys.argv[1]#"bert-base-uncased"

    tokenizer = AutoTokenizer.from_pretrained(f"{MODEL_NAME}/tokenizer")
    MAX_LEN=256

    class CustomDataset(torch.utils.data.Dataset):
        def __init__(self, df, tokenizer):
            self.tokenizer = tokenizer
            self.df = df
            self.sentence = list(self.df['text'])
            self.targets = self.df['label']
            self.max_len = MAX_LEN

        def __len__(self):
            return len(self.sentence)

        def __getitem__(self, index):
            sentence = str(self.sentence[index])
            sentence = " ".join(sentence.split())
            inputs = self.tokenizer.encode_plus(
                sentence,
                None,
                add_special_tokens=True,
                max_length=self.max_len,
                padding='max_length',
                return_token_type_ids=True,
                truncation=True,
                return_attention_mask=True,
                return_tensors='pt'
            )
            return {
                'input_ids': inputs['input_ids'].flatten(), # breaks any extra dimension as it is needed e.g. tensor(1) -> tensor()
                'attention_mask': inputs['attention_mask'].flatten(),
                'token_type_ids': inputs["token_type_ids"].flatten(),
                'labels': torch.FloatTensor(self.targets[index]),
            }

    train_dataset = CustomDataset(df_data, tokenizer)
    valid_dataset = CustomDataset(df_val, tokenizer)

    model = AutoModelForSequenceClassification.from_pretrained(f"{MODEL_NAME}/model",
                                                               problem_type="multi_label_classification",
                                                               num_labels=len(labels),
                                                               id2label=id2label,
                                                               label2id=label2id)

   # if torch.cuda.device_count()  >  1:
   #     model = torch.nn.DataParallel(model)
    model.to(device)

    """## Train the model!

    """

    batch_size = 60
    metric_name = "f1_macro"
    num_train_epochs = 20

    args = TrainingArguments(
        output_dir = f"{MODEL_NAME}",
        resume_from_checkpoint = True,
        evaluation_strategy = "epoch",
        save_strategy = "epoch",
        learning_rate=1e-5,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_train_epochs,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model=metric_name,
        auto_find_batch_size=True
        #push_to_hub=True,
    )

    """We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."""

    def compute_metrics(eval_prediction: EvalPrediction):
        prediction_scores, label_scores = eval_prediction
        sigmoid = torch.nn.Sigmoid()
        probs = sigmoid(torch.Tensor(prediction_scores))

        y_pred = np.zeros(probs.shape)
        y_pred[np.where(probs >= 0.5)] = 1.0
        y_true = label_scores

        metrics = {}
        try:
            metrics['f1_macro']= f1_score(y_true=y_true, y_pred=y_pred, average='macro')
            #metrics['f1_micro'] = f1_score(y_true=y_true, y_pred=y_pred, average='micro')
            #metrics['hamming'] = hamming_loss(y_true, y_pred)
            metrics['accuracy'] = accuracy_score(y_true, y_pred)
        except:
            raise AssertionError(f'deu ruim: {y_pred}-----{y_true}') 
        return metrics

    trainer = Trainer(
        model,
        args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        optimizers = (torch.optim.AdamW(model.parameters(), lr=1e-5, capturable=True), None)
    )

    trainer.train()

    """## Evaluate

    After training, we evaluate our model on the validation set.
    """

    trainer.evaluate()

    trainer.save_model(f"{MODEL_NAME}-fine-tuned/")
