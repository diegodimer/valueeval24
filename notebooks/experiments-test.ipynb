{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplt\n",
    "\n",
    "pyplt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "\n",
    "\n",
    "accuracy = {'bert-base-uncased': 0.501887569496877, 'bert-large-uncased': 0.48198229116617475, 'roberta-base': 0.4854828745967465, 'roberta-large': 0.5079277918868831, 'deberta-base': 0.47964856887912694, 'deberta-large': 0.5070354863065413}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{@{}lll@{}}\n",
      "\\toprule\n",
      "Model & Macro F1-Score \\\\\n",
      "\\midrule\n",
      "bert-base-uncased & 0.160 \\\\\n",
      "bert-large-uncased & 0.263 \\\\\n",
      "roberta-base & 0.248 \\\\\n",
      "roberta-large & 0.282 \\\\\n",
      "deberta-base & 0.274 \\\\\n",
      "deberta-large & 0.295 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'bert-base-uncased': 0.16032325537433148, 'bert-large-uncased': 0.262914881498199, 'roberta-base': 0.24838598997957248, 'roberta-large': 0.28230756868072876, 'deberta-base': 0.2738352572297258, 'deberta-large': 0.2950315205482525}\n",
    "\n",
    "for key in d:\n",
    "    d[key] = round(d[key], 3)\n",
    "df = pd.DataFrame(d.items(), columns=['Model', 'Macro F1-Score'])\n",
    "print(df.to_latex(float_format=\"%.3f\", index=False, column_format=\"@{}lll@{}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.read_csv(\"/Users/i530246/dev/mestrado/valueeval24/final_preds_test.csv\")\n",
    "final_preds.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "import ast\n",
    "# do literal_eval for each column except label and text\n",
    "for col in final_preds.columns:\n",
    "    if col not in [\"text\"]:\n",
    "        final_preds[col] = final_preds[col].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'bert-base-uncased-probs', 'bert-large-probs',\n",
       "       'roberta-base-probs', 'roberta-large-probs', 'deberta-base-probs',\n",
       "       'deberta-large-probs', 'bert-base-uncased-preds', 'bert-large-preds',\n",
       "       'roberta-base-preds', 'roberta-large-preds', 'deberta-base-preds',\n",
       "       'deberta-large-preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased 19\n",
      "bert-large-uncased 19\n",
      "roberta-base 19\n",
      "roberta-large 19\n",
      "deberta-base 19\n",
      "deberta-large 19\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.options.display.max_colwidth = 10000\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "macro_f1 = {'bert-base-uncased': 0.16032325537433148, 'bert-large': 0.262914881498199, 'roberta-base': 0.24838598997957248, 'roberta-large': 0.28230756868072876, 'deberta-base': 0.2738352572297258, 'deberta-large': 0.2950315205482525}\n",
    "\n",
    "\n",
    "macro_f1_models = {'bert-base-uncased': 0.16032325537433148, 'bert-large': 0.262914881498199, 'roberta-base': 0.24838598997957248, 'roberta-large': 0.28230756868072876, 'deberta-base': 0.2738352572297258, 'deberta-large': 0.2950315205482525}\n",
    "\n",
    "f1_scores_for_classes = {\n",
    "'bert-base-uncased': [ 0.         , 0.01506591 , 0.15062762 , 0.04545455 , 0.2820339  , 0.22064777\n",
    " , 0.30110701 , 0.02173913 , 0.17879418 , 0.37267081 , 0.05882353 , 0.38655462\n",
    " , 0.         , 0.         , 0.09638554 , 0.08536585 , 0.31814416 , 0.51272727\n",
    " , 0.        ],\n",
    "'bert-large-uncased' :[ 0.0661157  , 0.18534483 , 0.22822823 , 0.25112108 , 0.3409759  , 0.26325758\n",
    " , 0.31552795 , 0.16548463 , 0.30595813 , 0.41158926 , 0.37460317 , 0.42052426\n",
    " , 0.09584665 , 0.         , 0.2269289  , 0.25363825 , 0.3597519  , 0.57886179\n",
    " , 0.15162455],\n",
    "'roberta-base': [ 0.02816901 , 0.1618123  , 0.21474359 , 0.22335025 , 0.31937799 , 0.27968338\n",
    " , 0.26893354 , 0.20792079 , 0.27608347 , 0.36363636 , 0.40522876 , 0.37645616\n",
    " , 0.09219858 , 0.         , 0.19420784 , 0.24796748 , 0.35302293 , 0.54159292\n",
    " , 0.16494845],\n",
    "'roberta-large': [0.06866953 , 0.18669778 , 0.2539185  , 0.24630542 , 0.36398467 , 0.26470588\n",
    " , 0.32155732 , 0.23       , 0.31195841 , 0.41121495 , 0.46540881 , 0.41522903\n",
    " , 0.12903226 , 0.04545455 , 0.25503356 , 0.28215768 , 0.38180462 , 0.5658363\n",
    " , 0.16487455],\n",
    "'deberta-base': [0.09205021 , 0.18390805 , 0.22809458 , 0.26724138 , 0.33534379 , 0.26086957\n",
    " , 0.3226213  , 0.20895522 , 0.32534247 , 0.37056976 , 0.41520468 , 0.41760722\n",
    " , 0.15724816 , 0.         , 0.21981982 , 0.25779626 , 0.33048875 , 0.55574043\n",
    " , 0.25396825],\n",
    "'deberta-large': [ 0.096      , 0.21604278 , 0.28612717 , 0.30927835 , 0.36672968 , 0.29990967\n",
    " , 0.29600626 , 0.25615764 , 0.37717908 , 0.39110287 , 0.44680851 , 0.42838371\n",
    " , 0.09364548 , 0.         , 0.22809917 , 0.29025845 , 0.38314176 , 0.60124611\n",
    " , 0.2394822 ]\n",
    "}\n",
    "\n",
    "for i, j in f1_scores_for_classes.items():\n",
    "    print(i, len(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n"
     ]
    }
   ],
   "source": [
    "final_preds.columns\n",
    "### EQUAL VOTING\n",
    "## sum all -probs columns and divide by the number of columns. Then apply the threshold of 0.5 and compare with the label\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "    acc_col = [ 1 if x/6 > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['equal-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-equal'] = round(f1_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist(), average='macro', zero_division=0),3)\n",
    "f1_scores_for_classes['prob-equal'] = f1_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-equal'] = accuracy_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist())\n",
    "print(macro_f1['prob-equal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.326\n"
     ]
    }
   ],
   "source": [
    "final_preds.columns\n",
    "## WEIGHTED VOTING. Large models vote double than base models\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            if \"base\" in col:\n",
    "                acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "            if 'large' in col:\n",
    "                acc_col = [acc_col[i] + 2*row[col][i] for i in range(19)]\n",
    "    acc_col = [ 1 if (x/9) > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['large-double-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-large-double'] = round(f1_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist(), average='macro', zero_division=0),3)\n",
    "f1_scores_for_classes['prob-large-double'] = f1_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-large-double'] = accuracy_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist())\n",
    "print(macro_f1['prob-large-double'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n"
     ]
    }
   ],
   "source": [
    "## WEIGTHED VOTING. Models votes with their macro f1-score\n",
    "d = {}\n",
    "\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            model = col.replace(\"-probs\", \"\")\n",
    "            acc_col = [acc_col[i] + macro_f1_models[model]*row[col][i] for i in range(19)]\n",
    "    acc_col = [float(x / sum(macro_f1_models.values())) for x in acc_col]\n",
    "    acc_col = [1 if x > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['f1-score-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-weight-macro-f1'] = round(f1_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['prob-weight-macro-f1'] = f1_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-weight-macro-f1'] = accuracy_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist())\n",
    "print(macro_f1['prob-weight-macro-f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.318\n"
     ]
    }
   ],
   "source": [
    "## MAJORITY VOTING. On preds, the majority wins\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-preds\" in col:\n",
    "            acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "    acc_col = [1 if x > 1 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['majority-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['preds-majority'] = round(f1_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['preds-majority'] = f1_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['preds-majority'] = accuracy_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist())\n",
    "print(macro_f1['preds-majority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.319\n"
     ]
    }
   ],
   "source": [
    "## MAJORITY VOTING, DOUBLE FOR LARGE MODELS. On preds, the majority wins\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-preds\" in col:\n",
    "            if \"base\" in col:\n",
    "                acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "            if 'large' in col:\n",
    "                acc_col = [acc_col[i] + 2*row[col][i] for i in range(19)]\n",
    "    acc_col = [1 if x > 1 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['majority-large-double-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['preds-large-double'] = round(f1_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['preds-large-double'] = f1_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['preds-large-double'] = accuracy_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist())\n",
    "print(macro_f1['preds-large-double'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Model & Macro F1-Score \\\\\n",
      "\\midrule\n",
      "bert-base-uncased & 0.160 \\\\\n",
      "bert-large & 0.263 \\\\\n",
      "roberta-base & 0.248 \\\\\n",
      "roberta-large & 0.282 \\\\\n",
      "deberta-base & 0.274 \\\\\n",
      "deberta-large & 0.295 \\\\\n",
      "prob-equal & 0.330 \\\\\n",
      "prob-large-double & 0.326 \\\\\n",
      "prob-weight-macro-f1 & 0.330 \\\\\n",
      "preds-majority & 0.318 \\\\\n",
      "preds-large-double & 0.319 \\\\\n",
      "Hierocles of Alexandria $\\dagger$ & {\\cellcolor{gray!25}} 0.390 \\\\\n",
      "Arthur Schopenhauer $\\star$ $\\dagger$ & 0.350 \\\\\n",
      "Philo of Alexandria & 0.280 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_19181/3866228948.py:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  macro_f1['Hierocles of Alexandria $\\dagger$'] = 0.39\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_19181/3866228948.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  macro_f1['Arthur Schopenhauer $\\star$ $\\dagger$'] = 0.35\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_19181/3866228948.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  .to_latex(convert_css=True, hrules=True, caption=\"Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version\",position_float='centering')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "macro_f1['Hierocles of Alexandria $\\dagger$'] = 0.39 \n",
    "macro_f1['Arthur Schopenhauer $\\star$ $\\dagger$'] = 0.35 \n",
    "macro_f1['Philo of Alexandria'] = 0.28\n",
    "df = pd.DataFrame(macro_f1.items(), columns=['Model', 'Macro F1-Score'])\n",
    "\n",
    "print ( df.style\\\n",
    ".highlight_max(color='gray!25', subset=['Macro F1-Score'] ).hide(level=0, axis=0) \\\n",
    ".format(\"{:.3f}\", subset=['Macro F1-Score']) \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version\",position_float='centering')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance']\n",
    "\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for f1_scores_for_classes using the existing dict and mapping the ids to labels\n",
    "f1_scores_for_classes_df = pd.DataFrame(f1_scores_for_classes, index=[id2label[i] for i in range(19)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{F1-Scores for each human value}\n",
      "\\begin{tabular}{lrrrrrrrrrrr}\n",
      "\\toprule\n",
      " & \\rotatebox{90}{bert-base-uncased} & \\rotatebox{90}{bert-large-uncased} & \\rotatebox{90}{roberta-base} & \\rotatebox{90}{roberta-large} & \\rotatebox{90}{deberta-base} & \\rotatebox{90}{deberta-large} & \\rotatebox{90}{prob-equal} & \\rotatebox{90}{prob-large-double} & \\rotatebox{90}{prob-weight-macro-f1} & \\rotatebox{90}{preds-majority} & \\rotatebox{90}{preds-large-double} \\\\\n",
      "\\midrule\n",
      "Self-direction: thought & 00 & 07 & 03 & 07 & 09 & 10 & 11 & 11 & {\\cellcolor{gray!25}} 12 & 06 & 12 \\\\\n",
      "Self-direction: action & 02 & 19 & 16 & 19 & 18 & 22 & {\\cellcolor{gray!25}} 23 & 23 & 23 & 23 & 23 \\\\\n",
      "Stimulation & 15 & 23 & 21 & 25 & 23 & 29 & 30 & {\\cellcolor{gray!25}} 30 & 30 & 30 & 29 \\\\\n",
      "Hedonism & 05 & 25 & 22 & 25 & 27 & 31 & 31 & {\\cellcolor{gray!25}} 33 & 32 & 33 & 33 \\\\\n",
      "Achievement & 28 & 34 & 32 & 36 & 34 & 37 & {\\cellcolor{gray!25}} 40 & 40 & 40 & 40 & 39 \\\\\n",
      "Power: dominance & 22 & 26 & 28 & 26 & 26 & 30 & {\\cellcolor{gray!25}} 36 & 35 & 35 & 33 & 34 \\\\\n",
      "Power: resources & 30 & 32 & 27 & 32 & 32 & 30 & 39 & 38 & {\\cellcolor{gray!25}} 39 & 37 & 38 \\\\\n",
      "Face & 02 & 17 & 21 & 23 & 21 & 26 & {\\cellcolor{gray!25}} 30 & 29 & 30 & 29 & 29 \\\\\n",
      "Security: personal & 18 & 31 & 28 & 31 & 33 & 38 & 37 & 38 & {\\cellcolor{gray!25}} 38 & 37 & 37 \\\\\n",
      "Security: societal & 37 & 41 & 36 & 41 & 37 & 39 & {\\cellcolor{gray!25}} 44 & 44 & 44 & 44 & 44 \\\\\n",
      "Tradition & 06 & 37 & 41 & 47 & 42 & 45 & 50 & 46 & 49 & {\\cellcolor{gray!25}} 51 & 44 \\\\\n",
      "Conformity: rules & 39 & 42 & 38 & 42 & 42 & 43 & 46 & 46 & 46 & {\\cellcolor{gray!25}} 46 & 45 \\\\\n",
      "Conformity: interpersonal & 00 & 10 & 09 & 13 & 16 & 09 & {\\cellcolor{gray!25}} 17 & 16 & 16 & 14 & 15 \\\\\n",
      "Humility & 00 & 00 & 00 & 05 & 00 & 00 & {\\cellcolor{gray!25}} 05 & 05 & {\\cellcolor{gray!25}} 05 & 00 & 04 \\\\\n",
      "Benevolence: caring & 10 & 23 & 19 & 26 & 22 & 23 & {\\cellcolor{gray!25}} 29 & 27 & 28 & 26 & 25 \\\\\n",
      "Benevolence: dependability & 09 & 25 & 25 & 28 & 26 & 29 & 33 & 33 & 33 & 34 & {\\cellcolor{gray!25}} 34 \\\\\n",
      "Universalism: concern & 32 & 36 & 35 & 38 & 33 & 38 & 42 & 42 & {\\cellcolor{gray!25}} 42 & 41 & 41 \\\\\n",
      "Universalism: nature & 51 & 58 & 54 & 57 & 56 & {\\cellcolor{gray!25}} 60 & 59 & 59 & 59 & 58 & 59 \\\\\n",
      "Universalism: tolerance & 00 & 15 & 16 & 16 & {\\cellcolor{gray!25}} 25 & 24 & 25 & 24 & 25 & 23 & 22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_format = lambda x: f\"{x:.2f}\".split('.')[1]\n",
    "\n",
    "latex_table = f1_scores_for_classes_df.style \\\n",
    ".highlight_max(color='gray!25', axis=1) \\\n",
    ".format(float_format, subset=f1_scores_for_classes_df.columns)   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"F1-Scores for each human value\",position_float='centering')\n",
    "\n",
    "for column in f1_scores_for_classes_df.columns:\n",
    "    latex_table = latex_table.replace(column, f'\\\\rotatebox{{90}}{{{column}}}')\n",
    "\n",
    "print(latex_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hierocles of Alexandria</th>\n",
       "      <th>Arthur Schopenhauer</th>\n",
       "      <th>Philo of Alexandria</th>\n",
       "      <th>prob-weight-macro-f1</th>\n",
       "      <th>preds-large-double</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.846690</td>\n",
       "      <td>11.594203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self-direction: action</th>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.030303</td>\n",
       "      <td>22.533137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stimulation</th>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.391061</td>\n",
       "      <td>28.898129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedonism</th>\n",
       "      <td>37.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.617647</td>\n",
       "      <td>32.525952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Achievement</th>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>39.905882</td>\n",
       "      <td>39.444195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: dominance</th>\n",
       "      <td>42.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.277383</td>\n",
       "      <td>34.123223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: resources</th>\n",
       "      <td>49.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.317124</td>\n",
       "      <td>37.735849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Face</th>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.827916</td>\n",
       "      <td>28.621291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: personal</th>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>38.373571</td>\n",
       "      <td>36.763006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: societal</th>\n",
       "      <td>49.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.153226</td>\n",
       "      <td>43.752029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tradition</th>\n",
       "      <td>46.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>49.376559</td>\n",
       "      <td>44.494382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: rules</th>\n",
       "      <td>51.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>45.761167</td>\n",
       "      <td>44.781445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.475973</td>\n",
       "      <td>15.151515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humility</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>4.255319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.019324</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.755906</td>\n",
       "      <td>33.973412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: concern</th>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.099386</td>\n",
       "      <td>40.792291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: nature</th>\n",
       "      <td>63.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.135286</td>\n",
       "      <td>58.728011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.873096</td>\n",
       "      <td>22.276029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Hierocles of Alexandria  Arthur Schopenhauer  Philo of Alexandria  prob-weight-macro-f1  preds-large-double\n",
       "Self-direction: thought                        15.0                 12.0                  8.0             11.846690           11.594203\n",
       "Self-direction: action                         27.0                 24.0                 22.0             23.030303           22.533137\n",
       "Stimulation                                    30.0                 33.0                 27.0             30.391061           28.898129\n",
       "Hedonism                                       37.0                 35.0                 31.0             31.617647           32.525952\n",
       "Achievement                                    45.0                 40.0                 35.0             39.905882           39.444195\n",
       "Power: dominance                               42.0                 37.0                 31.0             35.277383           34.123223\n",
       "Power: resources                               49.0                 47.0                 34.0             39.317124           37.735849\n",
       "Face                                           31.0                 24.0                 17.0             29.827916           28.621291\n",
       "Security: personal                             42.0                 38.0                 33.0             38.373571           36.763006\n",
       "Security: societal                             49.0                 46.0                 40.0             44.153226           43.752029\n",
       "Tradition                                      46.0                 49.0                 47.0             49.376559           44.494382\n",
       "Conformity: rules                              51.0                 50.0                 42.0             45.761167           44.781445\n",
       "Conformity: interpersonal                      24.0                 19.0                  9.0             16.475973           15.151515\n",
       "Humility                                        0.0                  0.0                  0.0              5.405405            4.255319\n",
       "Benevolence: caring                            34.0                 32.0                 21.0             28.019324           25.000000\n",
       "Benevolence: dependability                     33.0                 31.0                 28.0             32.755906           33.973412\n",
       "Universalism: concern                          47.0                 46.0                 40.0             42.099386           40.792291\n",
       "Universalism: nature                           63.0                 60.0                 57.0             59.135286           58.728011\n",
       "Universalism: tolerance                        27.0                 27.0                 21.0             24.873096           22.276029"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores_for_classes_2 = {\n",
    "'Hierocles of Alexandria': [ 15., 27., 30., 37., 45., 42., 49., 31., 42., 49., 46., 51., 24., 00., 34., 33., 47., 63., 27],\n",
    "'Arthur Schopenhauer':     [12., 24., 33., 35., 40., 37., 47., 24., 38., 46., 49., 50., 19., 00., 32., 31., 46., 60., 27],\n",
    "'Philo of Alexandria':     [ 8., 22., 27., 31., 35., 31., 34., 17., 33., 40., 47., 42., 9., 00., 21., 28., 40., 57., 21],\n",
    "'prob-weight-macro-f1':    [x*100 for x in f1_scores_for_classes['prob-weight-macro-f1']],\n",
    "'preds-large-double':      [x*100 for x in f1_scores_for_classes['preds-large-double']]\n",
    "}\n",
    "\n",
    "f1_general = pd.DataFrame(f1_scores_for_classes_2, index=[id2label[i] for i in range(19)])\n",
    "f1_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{F1-Scores for each human value}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      " & \\rotatebox{90}{Hierocles of Alexandria} & \\rotatebox{90}{Arthur Schopenhauer} & \\rotatebox{90}{Philo of Alexandria} & \\rotatebox{90}{prob-weight-macro-f1} & \\rotatebox{90}{preds-large-double} \\\\\n",
      "\\midrule\n",
      "Self-direction: thought & {\\cellcolor{gray!25}} 15 & 12 & 8 & 11 & 11 \\\\\n",
      "Self-direction: action & {\\cellcolor{gray!25}} 27 & 24 & 22 & 23 & 22 \\\\\n",
      "Stimulation & 30 & {\\cellcolor{gray!25}} 33 & 27 & 30 & 28 \\\\\n",
      "Hedonism & {\\cellcolor{gray!25}} 37 & 35 & 31 & 31 & 32 \\\\\n",
      "Achievement & {\\cellcolor{gray!25}} 45 & 40 & 35 & 39 & 39 \\\\\n",
      "Power: dominance & {\\cellcolor{gray!25}} 42 & 37 & 31 & 35 & 34 \\\\\n",
      "Power: resources & {\\cellcolor{gray!25}} 49 & 47 & 34 & 39 & 37 \\\\\n",
      "Face & {\\cellcolor{gray!25}} 31 & 24 & 17 & 29 & 28 \\\\\n",
      "Security: personal & {\\cellcolor{gray!25}} 42 & 38 & 33 & 38 & 36 \\\\\n",
      "Security: societal & {\\cellcolor{gray!25}} 49 & 46 & 40 & 44 & 43 \\\\\n",
      "Tradition & 46 & 49 & 47 & {\\cellcolor{gray!25}} 49 & 44 \\\\\n",
      "Conformity: rules & {\\cellcolor{gray!25}} 51 & 50 & 42 & 45 & 44 \\\\\n",
      "Conformity: interpersonal & {\\cellcolor{gray!25}} 24 & 19 & 9 & 16 & 15 \\\\\n",
      "Humility & 0 & 0 & 0 & {\\cellcolor{gray!25}} 5 & 4 \\\\\n",
      "Benevolence: caring & {\\cellcolor{gray!25}} 34 & 32 & 21 & 28 & 25 \\\\\n",
      "Benevolence: dependability & 33 & 31 & 28 & 32 & {\\cellcolor{gray!25}} 33 \\\\\n",
      "Universalism: concern & {\\cellcolor{gray!25}} 47 & 46 & 40 & 42 & 40 \\\\\n",
      "Universalism: nature & {\\cellcolor{gray!25}} 63 & 60 & 57 & 59 & 58 \\\\\n",
      "Universalism: tolerance & {\\cellcolor{gray!25}} 27 & {\\cellcolor{gray!25}} 27 & 21 & 24 & 22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_format = lambda x: f\"{x:.02f}\".split('.')[0]\n",
    "\n",
    "latex_table = f1_general.style \\\n",
    ".highlight_max(color='gray!25', axis=1) \\\n",
    ".format(float_format, subset=f1_general.columns)   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"F1-Scores for each human value\",position_float='centering')\n",
    "\n",
    "for column in f1_general.columns:\n",
    "    latex_table = latex_table.replace(column, f'\\\\rotatebox{{90}}{{{column}}}')\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Accuracy Scores for the trained models}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Model & Accuracy \\\\\n",
      "\\midrule\n",
      "roberta-large & {\\cellcolor{gray!25}} 0.508 \\\\\n",
      "deberta-large & 0.507 \\\\\n",
      "bert-base-uncased & 0.502 \\\\\n",
      "roberta-base & 0.485 \\\\\n",
      "preds-majority & 0.484 \\\\\n",
      "bert-large-uncased & 0.482 \\\\\n",
      "deberta-base & 0.480 \\\\\n",
      "prob-equal & 0.447 \\\\\n",
      "prob-weight-macro-f1 & 0.445 \\\\\n",
      "prob-large-double & 0.438 \\\\\n",
      "preds-large-double & 0.418 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = dict(sorted(accuracy.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy.items(), columns=['Model', 'Accuracy']).style \\\n",
    ".highlight_max(color='gray!25', subset=['Accuracy'] ).hide(level=0, axis=0) \\\n",
    ".format(\"{:.3f}\", subset=['Accuracy'])   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"Accuracy Scores for the trained models\",position_float='centering')\n",
    "print(accuracy_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
