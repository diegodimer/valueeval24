{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplt\n",
    "\n",
    "pyplt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "\n",
    "\n",
    "accuracy = {'bert-base-uncased': 0.501887569496877, 'bert-large-uncased': 0.48198229116617475, 'roberta-base': 0.4854828745967465, 'roberta-large': 0.5079277918868831, 'deberta-base': 0.47964856887912694, 'deberta-large': 0.5070354863065413}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{@{}lll@{}}\n",
      "\\toprule\n",
      "Model & Macro F1-Score \\\\\n",
      "\\midrule\n",
      "bert-base-uncased & 0.160 \\\\\n",
      "bert-large-uncased & 0.263 \\\\\n",
      "roberta-base & 0.248 \\\\\n",
      "roberta-large & 0.282 \\\\\n",
      "deberta-base & 0.274 \\\\\n",
      "deberta-large & 0.295 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'bert-base-uncased': 0.16032325537433148, 'bert-large-uncased': 0.262914881498199, 'roberta-base': 0.24838598997957248, 'roberta-large': 0.28230756868072876, 'deberta-base': 0.2738352572297258, 'deberta-large': 0.2950315205482525}\n",
    "\n",
    "for key in d:\n",
    "    d[key] = round(d[key], 3)\n",
    "df = pd.DataFrame(d.items(), columns=['Model', 'Macro F1-Score'])\n",
    "print(df.to_latex(float_format=\"%.3f\", index=False, column_format=\"@{}lll@{}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.read_csv(\"/Users/i530246/dev/mestrado/valueeval24/final_preds_test.csv\")\n",
    "final_preds.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "import ast\n",
    "# do literal_eval for each column except label and text\n",
    "for col in final_preds.columns:\n",
    "    if col not in [\"text\"]:\n",
    "        final_preds[col] = final_preds[col].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'bert-base-uncased-probs', 'bert-large-probs',\n",
       "       'roberta-base-probs', 'roberta-large-probs', 'deberta-base-probs',\n",
       "       'deberta-large-probs', 'bert-base-uncased-preds', 'bert-large-preds',\n",
       "       'roberta-base-preds', 'roberta-large-preds', 'deberta-base-preds',\n",
       "       'deberta-large-preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased 19\n",
      "bert-large-uncased 19\n",
      "roberta-base 19\n",
      "roberta-large 19\n",
      "deberta-base 19\n",
      "deberta-large 19\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.options.display.max_colwidth = 10000\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "macro_f1 = {'bert-base-uncased': 0.16032325537433148, 'bert-large': 0.262914881498199, 'roberta-base': 0.24838598997957248, 'roberta-large': 0.28230756868072876, 'deberta-base': 0.2738352572297258, 'deberta-large': 0.2950315205482525}\n",
    "\n",
    "\n",
    "macro_f1_models = {'bert-base-uncased': 0.16032325537433148, 'bert-large': 0.262914881498199, 'roberta-base': 0.24838598997957248, 'roberta-large': 0.28230756868072876, 'deberta-base': 0.2738352572297258, 'deberta-large': 0.2950315205482525}\n",
    "\n",
    "f1_scores_for_classes = {\n",
    "'bert-base-uncased': [ 0.         , 0.01506591 , 0.15062762 , 0.04545455 , 0.2820339  , 0.22064777\n",
    " , 0.30110701 , 0.02173913 , 0.17879418 , 0.37267081 , 0.05882353 , 0.38655462\n",
    " , 0.         , 0.         , 0.09638554 , 0.08536585 , 0.31814416 , 0.51272727\n",
    " , 0.        ],\n",
    "'bert-large-uncased' :[ 0.0661157  , 0.18534483 , 0.22822823 , 0.25112108 , 0.3409759  , 0.26325758\n",
    " , 0.31552795 , 0.16548463 , 0.30595813 , 0.41158926 , 0.37460317 , 0.42052426\n",
    " , 0.09584665 , 0.         , 0.2269289  , 0.25363825 , 0.3597519  , 0.57886179\n",
    " , 0.15162455],\n",
    "'roberta-base': [ 0.02816901 , 0.1618123  , 0.21474359 , 0.22335025 , 0.31937799 , 0.27968338\n",
    " , 0.26893354 , 0.20792079 , 0.27608347 , 0.36363636 , 0.40522876 , 0.37645616\n",
    " , 0.09219858 , 0.         , 0.19420784 , 0.24796748 , 0.35302293 , 0.54159292\n",
    " , 0.16494845],\n",
    "'roberta-large': [0.06866953 , 0.18669778 , 0.2539185  , 0.24630542 , 0.36398467 , 0.26470588\n",
    " , 0.32155732 , 0.23       , 0.31195841 , 0.41121495 , 0.46540881 , 0.41522903\n",
    " , 0.12903226 , 0.04545455 , 0.25503356 , 0.28215768 , 0.38180462 , 0.5658363\n",
    " , 0.16487455],\n",
    "'deberta-base': [0.09205021 , 0.18390805 , 0.22809458 , 0.26724138 , 0.33534379 , 0.26086957\n",
    " , 0.3226213  , 0.20895522 , 0.32534247 , 0.37056976 , 0.41520468 , 0.41760722\n",
    " , 0.15724816 , 0.         , 0.21981982 , 0.25779626 , 0.33048875 , 0.55574043\n",
    " , 0.25396825],\n",
    "'deberta-large': [ 0.096      , 0.21604278 , 0.28612717 , 0.30927835 , 0.36672968 , 0.29990967\n",
    " , 0.29600626 , 0.25615764 , 0.37717908 , 0.39110287 , 0.44680851 , 0.42838371\n",
    " , 0.09364548 , 0.         , 0.22809917 , 0.29025845 , 0.38314176 , 0.60124611\n",
    " , 0.2394822 ]\n",
    "}\n",
    "\n",
    "for i, j in f1_scores_for_classes.items():\n",
    "    print(i, len(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n"
     ]
    }
   ],
   "source": [
    "final_preds.columns\n",
    "### EQUAL VOTING\n",
    "## sum all -probs columns and divide by the number of columns. Then apply the threshold of 0.5 and compare with the label\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "    acc_col = [ 1 if x/6 > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['equal-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-equal'] = round(f1_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist(), average='macro', zero_division=0),3)\n",
    "f1_scores_for_classes['prob-equal'] = f1_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-equal'] = accuracy_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist())\n",
    "print(macro_f1['prob-equal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.326\n"
     ]
    }
   ],
   "source": [
    "final_preds.columns\n",
    "## WEIGHTED VOTING. Large models vote double than base models\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            if \"base\" in col:\n",
    "                acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "            if 'large' in col:\n",
    "                acc_col = [acc_col[i] + 2*row[col][i] for i in range(19)]\n",
    "    acc_col = [ 1 if (x/9) > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['large-double-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-large-double'] = round(f1_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist(), average='macro', zero_division=0),3)\n",
    "f1_scores_for_classes['prob-large-double'] = f1_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist())\n",
    "print(macro_f1['prob-large-double'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n"
     ]
    }
   ],
   "source": [
    "## WEIGTHED VOTING. Models votes with their macro f1-score\n",
    "d = {}\n",
    "\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            model = col.replace(\"-probs\", \"\")\n",
    "            acc_col = [acc_col[i] + macro_f1_models[model]*row[col][i] for i in range(19)]\n",
    "    acc_col = [float(x / sum(macro_f1_models.values())) for x in acc_col]\n",
    "    acc_col = [1 if x > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['f1-score-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-weight-macro-f1'] = round(f1_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['prob-weight-macro-f1'] = f1_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-weight-macro-f1'] = accuracy_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist())\n",
    "print(macro_f1['prob-weight-macro-f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.318\n"
     ]
    }
   ],
   "source": [
    "## MAJORITY VOTING. On preds, the majority wins\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-preds\" in col:\n",
    "            acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "    acc_col = [1 if x > 1 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['majority-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['preds-majority'] = round(f1_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['preds-majority'] = f1_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['preds-majority'] = accuracy_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist())\n",
    "print(macro_f1['preds-majority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.319\n"
     ]
    }
   ],
   "source": [
    "## MAJORITY VOTING, DOUBLE FOR LARGE MODELS. On preds, the majority wins\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-preds\" in col:\n",
    "            if \"base\" in col:\n",
    "                acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "            if 'large' in col:\n",
    "                acc_col = [acc_col[i] + 2*row[col][i] for i in range(19)]\n",
    "    acc_col = [1 if x > 1 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['majority-large-double-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['preds-large-double'] = round(f1_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['preds-large-double'] = f1_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['preds-large-double'] = accuracy_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist())\n",
    "print(macro_f1['preds-large-double'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Model & Macro F1-Score \\\\\n",
      "\\midrule\n",
      "bert-base-uncased & 0.160 \\\\\n",
      "bert-large & 0.263 \\\\\n",
      "roberta-base & 0.248 \\\\\n",
      "roberta-large & 0.282 \\\\\n",
      "deberta-base & 0.274 \\\\\n",
      "deberta-large & 0.295 \\\\\n",
      "prob-equal & 0.330 \\\\\n",
      "prob-large-double & 0.323 \\\\\n",
      "prob-weight-macro-f1 & 0.330 \\\\\n",
      "Arthur Schopenhauer $\\star$ $\\dagger$ & {\\cellcolor{gray!25}} 0.441 \\\\\n",
      "Philo of Alexandria & 0.200 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_81432/513005734.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  macro_f1['Arthur Schopenhauer $\\star$ $\\dagger$'] = 0.4405\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_81432/513005734.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  .to_latex(convert_css=True, hrules=True, caption=\"Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version\",position_float='centering')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# macro_f1['Hierocles of Alexandria $\\dagger$'] = 0.39 \n",
    "macro_f1['Arthur Schopenhauer $\\star$ $\\dagger$'] = 0.4405 \n",
    "macro_f1['Philo of Alexandria'] = 0.20\n",
    "df = pd.DataFrame(macro_f1.items(), columns=['Model', 'Macro F1-Score'])\n",
    "\n",
    "print ( df.style\\\n",
    ".highlight_max(color='gray!25', subset=['Macro F1-Score'] ).hide(level=0, axis=0) \\\n",
    ".format(\"{:.3f}\", subset=['Macro F1-Score']) \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version\",position_float='centering')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance']\n",
    "\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for f1_scores_for_classes using the existing dict and mapping the ids to labels\n",
    "f1_scores_for_classes_df = pd.DataFrame(f1_scores_for_classes, index=[id2label[i] for i in range(19)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{F1-Scores for each human value}\n",
      "\\begin{tabular}{lrrrrrrrrr}\n",
      "\\toprule\n",
      " & \\rotatebox{90}{bert-base-uncased} & \\rotatebox{90}{bert-large-uncased} & \\rotatebox{90}{roberta-base} & \\rotatebox{90}{roberta-large} & \\rotatebox{90}{deberta-base} & \\rotatebox{90}{deberta-large} & \\rotatebox{90}{prob-equal} & \\rotatebox{90}{prob-large-double} & \\rotatebox{90}{prob-weight-macro-f1} \\\\\n",
      "\\midrule\n",
      "Self-direction: thought & 00 & 07 & 03 & 07 & 09 & 10 & 11 & 09 & {\\cellcolor{gray!25}} 12 \\\\\n",
      "Self-direction: action & 02 & 19 & 16 & 19 & 18 & 22 & {\\cellcolor{gray!25}} 23 & 23 & 23 \\\\\n",
      "Stimulation & 15 & 23 & 21 & 25 & 23 & 29 & 30 & 30 & {\\cellcolor{gray!25}} 30 \\\\\n",
      "Hedonism & 05 & 25 & 22 & 25 & 27 & 31 & 31 & 30 & {\\cellcolor{gray!25}} 32 \\\\\n",
      "Achievement & 28 & 34 & 32 & 36 & 34 & 37 & 40 & {\\cellcolor{gray!25}} 40 & 40 \\\\\n",
      "Power: dominance & 22 & 26 & 28 & 26 & 26 & 30 & {\\cellcolor{gray!25}} 36 & 34 & 35 \\\\\n",
      "Power: resources & 30 & 32 & 27 & 32 & 32 & 30 & 39 & 36 & {\\cellcolor{gray!25}} 39 \\\\\n",
      "Face & 02 & 17 & 21 & 23 & 21 & 26 & {\\cellcolor{gray!25}} 30 & 29 & 30 \\\\\n",
      "Security: personal & 18 & 31 & 28 & 31 & 33 & 38 & 37 & 38 & {\\cellcolor{gray!25}} 38 \\\\\n",
      "Security: societal & 37 & 41 & 36 & 41 & 37 & 39 & 44 & {\\cellcolor{gray!25}} 45 & 44 \\\\\n",
      "Tradition & 06 & 37 & 41 & 47 & 42 & 45 & 50 & {\\cellcolor{gray!25}} 53 & 49 \\\\\n",
      "Conformity: rules & 39 & 42 & 38 & 42 & 42 & 43 & 46 & {\\cellcolor{gray!25}} 46 & 46 \\\\\n",
      "Conformity: interpersonal & 00 & 10 & 09 & 13 & 16 & 09 & {\\cellcolor{gray!25}} 17 & 14 & 16 \\\\\n",
      "Humility & 00 & 00 & 00 & 05 & 00 & 00 & 05 & {\\cellcolor{gray!25}} 06 & 05 \\\\\n",
      "Benevolence: caring & 10 & 23 & 19 & 26 & 22 & 23 & {\\cellcolor{gray!25}} 29 & 27 & 28 \\\\\n",
      "Benevolence: dependability & 09 & 25 & 25 & 28 & 26 & 29 & {\\cellcolor{gray!25}} 33 & 33 & 33 \\\\\n",
      "Universalism: concern & 32 & 36 & 35 & 38 & 33 & 38 & 42 & 42 & {\\cellcolor{gray!25}} 42 \\\\\n",
      "Universalism: nature & 51 & 58 & 54 & 57 & 56 & {\\cellcolor{gray!25}} 60 & 59 & 59 & 59 \\\\\n",
      "Universalism: tolerance & 00 & 15 & 16 & 16 & {\\cellcolor{gray!25}} 25 & 24 & 25 & 21 & 25 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_format = lambda x: f\"{x:.2f}\".split('.')[1]\n",
    "\n",
    "latex_table = f1_scores_for_classes_df.style \\\n",
    ".highlight_max(color='gray!25', axis=1) \\\n",
    ".format(float_format, subset=f1_scores_for_classes_df.columns)   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"F1-Scores for each human value\",position_float='centering')\n",
    "\n",
    "for column in f1_scores_for_classes_df.columns:\n",
    "    latex_table = latex_table.replace(column, f'\\\\rotatebox{{90}}{{{column}}}')\n",
    "\n",
    "print(latex_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'preds-large-double'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m f1_scores_for_classes_2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHierocles of Alexandria\u001b[39m\u001b[38;5;124m'\u001b[39m: [ \u001b[38;5;241m15.\u001b[39m, \u001b[38;5;241m27.\u001b[39m, \u001b[38;5;241m30.\u001b[39m, \u001b[38;5;241m37.\u001b[39m, \u001b[38;5;241m45.\u001b[39m, \u001b[38;5;241m42.\u001b[39m, \u001b[38;5;241m49.\u001b[39m, \u001b[38;5;241m31.\u001b[39m, \u001b[38;5;241m42.\u001b[39m, \u001b[38;5;241m49.\u001b[39m, \u001b[38;5;241m46.\u001b[39m, \u001b[38;5;241m51.\u001b[39m, \u001b[38;5;241m24.\u001b[39m, \u001b[38;5;241m00.\u001b[39m, \u001b[38;5;241m34.\u001b[39m, \u001b[38;5;241m33.\u001b[39m, \u001b[38;5;241m47.\u001b[39m, \u001b[38;5;241m63.\u001b[39m, \u001b[38;5;241m27\u001b[39m],\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArthur Schopenhauer\u001b[39m\u001b[38;5;124m'\u001b[39m:     [\u001b[38;5;241m12.\u001b[39m, \u001b[38;5;241m24.\u001b[39m, \u001b[38;5;241m33.\u001b[39m, \u001b[38;5;241m35.\u001b[39m, \u001b[38;5;241m40.\u001b[39m, \u001b[38;5;241m37.\u001b[39m, \u001b[38;5;241m47.\u001b[39m, \u001b[38;5;241m24.\u001b[39m, \u001b[38;5;241m38.\u001b[39m, \u001b[38;5;241m46.\u001b[39m, \u001b[38;5;241m49.\u001b[39m, \u001b[38;5;241m50.\u001b[39m, \u001b[38;5;241m19.\u001b[39m, \u001b[38;5;241m00.\u001b[39m, \u001b[38;5;241m32.\u001b[39m, \u001b[38;5;241m31.\u001b[39m, \u001b[38;5;241m46.\u001b[39m, \u001b[38;5;241m60.\u001b[39m, \u001b[38;5;241m27\u001b[39m],\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhilo of Alexandria\u001b[39m\u001b[38;5;124m'\u001b[39m:     [ \u001b[38;5;241m8.\u001b[39m, \u001b[38;5;241m22.\u001b[39m, \u001b[38;5;241m27.\u001b[39m, \u001b[38;5;241m31.\u001b[39m, \u001b[38;5;241m35.\u001b[39m, \u001b[38;5;241m31.\u001b[39m, \u001b[38;5;241m34.\u001b[39m, \u001b[38;5;241m17.\u001b[39m, \u001b[38;5;241m33.\u001b[39m, \u001b[38;5;241m40.\u001b[39m, \u001b[38;5;241m47.\u001b[39m, \u001b[38;5;241m42.\u001b[39m, \u001b[38;5;241m9.\u001b[39m, \u001b[38;5;241m00.\u001b[39m, \u001b[38;5;241m21.\u001b[39m, \u001b[38;5;241m28.\u001b[39m, \u001b[38;5;241m40.\u001b[39m, \u001b[38;5;241m57.\u001b[39m, \u001b[38;5;241m21\u001b[39m],\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprob-weight-macro-f1\u001b[39m\u001b[38;5;124m'\u001b[39m:    [x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f1_scores_for_classes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprob-weight-macro-f1\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds-large-double\u001b[39m\u001b[38;5;124m'\u001b[39m:      [x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mf1_scores_for_classes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreds-large-double\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m f1_general \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(f1_scores_for_classes_2, index\u001b[38;5;241m=\u001b[39m[id2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m19\u001b[39m)])\n\u001b[1;32m     10\u001b[0m f1_general\n",
      "\u001b[0;31mKeyError\u001b[0m: 'preds-large-double'"
     ]
    }
   ],
   "source": [
    "f1_scores_for_classes_2 = {\n",
    "'Hierocles of Alexandria': [ 15., 27., 30., 37., 45., 42., 49., 31., 42., 49., 46., 51., 24., 00., 34., 33., 47., 63., 27],\n",
    "'Arthur Schopenhauer':     [12., 24., 33., 35., 40., 37., 47., 24., 38., 46., 49., 50., 19., 00., 32., 31., 46., 60., 27],\n",
    "'Philo of Alexandria':     [ 8., 22., 27., 31., 35., 31., 34., 17., 33., 40., 47., 42., 9., 00., 21., 28., 40., 57., 21],\n",
    "'prob-weight-macro-f1':    [x*100 for x in f1_scores_for_classes['prob-weight-macro-f1']],\n",
    "'preds-large-double':      [x*100 for x in f1_scores_for_classes['preds-large-double']]\n",
    "}\n",
    "\n",
    "f1_general = pd.DataFrame(f1_scores_for_classes_2, index=[id2label[i] for i in range(19)])\n",
    "f1_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{F1-Scores for each human value}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      " & \\rotatebox{90}{Hierocles of Alexandria} & \\rotatebox{90}{Arthur Schopenhauer} & \\rotatebox{90}{Philo of Alexandria} & \\rotatebox{90}{prob-weight-macro-f1} & \\rotatebox{90}{preds-large-double} \\\\\n",
      "\\midrule\n",
      "Self-direction: thought & {\\cellcolor{gray!25}} 15 & 12 & 8 & 14 & 13 \\\\\n",
      "Self-direction: action & {\\cellcolor{gray!25}} 27 & 24 & 22 & 24 & 25 \\\\\n",
      "Stimulation & 30 & 33 & 27 & 34 & {\\cellcolor{gray!25}} 34 \\\\\n",
      "Hedonism & 37 & 35 & 31 & {\\cellcolor{gray!25}} 37 & 37 \\\\\n",
      "Achievement & {\\cellcolor{gray!25}} 45 & 40 & 35 & 40 & 39 \\\\\n",
      "Power: dominance & {\\cellcolor{gray!25}} 42 & 37 & 31 & 35 & 34 \\\\\n",
      "Power: resources & {\\cellcolor{gray!25}} 49 & 47 & 34 & 33 & 32 \\\\\n",
      "Face & 31 & 24 & 17 & {\\cellcolor{gray!25}} 31 & 29 \\\\\n",
      "Security: personal & {\\cellcolor{gray!25}} 42 & 38 & 33 & 36 & 35 \\\\\n",
      "Security: societal & {\\cellcolor{gray!25}} 49 & 46 & 40 & 44 & 45 \\\\\n",
      "Tradition & 46 & {\\cellcolor{gray!25}} 49 & 47 & 44 & 44 \\\\\n",
      "Conformity: rules & {\\cellcolor{gray!25}} 51 & 50 & 42 & 49 & 48 \\\\\n",
      "Conformity: interpersonal & {\\cellcolor{gray!25}} 24 & 19 & 9 & 20 & 17 \\\\\n",
      "Humility & 0 & 0 & 0 & {\\cellcolor{gray!25}} 4 & 0 \\\\\n",
      "Benevolence: caring & {\\cellcolor{gray!25}} 34 & 32 & 21 & 30 & 28 \\\\\n",
      "Benevolence: dependability & {\\cellcolor{gray!25}} 33 & 31 & 28 & 29 & 29 \\\\\n",
      "Universalism: concern & {\\cellcolor{gray!25}} 47 & 46 & 40 & 37 & 37 \\\\\n",
      "Universalism: nature & {\\cellcolor{gray!25}} 63 & 60 & 57 & 58 & 58 \\\\\n",
      "Universalism: tolerance & 27 & 27 & 21 & {\\cellcolor{gray!25}} 27 & 22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_format = lambda x: f\"{x:.02f}\".split('.')[0]\n",
    "\n",
    "latex_table = f1_general.style \\\n",
    ".highlight_max(color='gray!25', axis=1) \\\n",
    ".format(float_format, subset=f1_general.columns)   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"F1-Scores for each human value\",position_float='centering')\n",
    "\n",
    "for column in f1_general.columns:\n",
    "    latex_table = latex_table.replace(column, f'\\\\rotatebox{{90}}{{{column}}}')\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Accuracy Scores for the trained models}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Model & Accuracy \\\\\n",
      "\\midrule\n",
      "roberta-large & {\\cellcolor{gray!25}} 0.522 \\\\\n",
      "deberta-large & 0.520 \\\\\n",
      "bert-base-uncased & 0.513 \\\\\n",
      "preds-large-double & 0.503 \\\\\n",
      "roberta-base & 0.497 \\\\\n",
      "preds-majority & 0.493 \\\\\n",
      "deberta-base & 0.489 \\\\\n",
      "bert-large-uncased & 0.487 \\\\\n",
      "prob-equal & 0.452 \\\\\n",
      "prob-weight-macro-f1 & 0.451 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = dict(sorted(accuracy.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy.items(), columns=['Model', 'Accuracy']).style \\\n",
    ".highlight_max(color='gray!25', subset=['Accuracy'] ).hide(level=0, axis=0) \\\n",
    ".format(\"{:.3f}\", subset=['Accuracy'])   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"Accuracy Scores for the trained models\",position_float='centering')\n",
    "print(accuracy_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
