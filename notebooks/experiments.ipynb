{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplt\n",
    "\n",
    "pyplt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "\n",
    "accuracy = {'bert-base-uncased': 0.5126708646905237, 'bert-large-uncased': 0.48725234219013974, 'roberta-base': 0.49738903394255873, 'roberta-large': 0.5220396252495776, 'deberta-base': 0.4888649976962064, 'deberta-large': 0.5201965903855015}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{@{}lll@{}}\n",
      "\\toprule\n",
      "Model & Macro F1-Score \\\\\n",
      "\\midrule\n",
      "bert-base-uncased & 0.167 \\\\\n",
      "bert-large-uncased & 0.270 \\\\\n",
      "roberta-base & 0.253 \\\\\n",
      "roberta-large & 0.294 \\\\\n",
      "deberta-base & 0.269 \\\\\n",
      "deberta-large & 0.299 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'bert-base-uncased': 0.16691939082661103, 'bert-large-uncased': 0.2695939816828627, 'roberta-base': 0.25323999340304165, 'roberta-large': 0.29354155627190104, 'deberta-base': 0.26874974711093474, 'deberta-large': 0.2988116271661447}\n",
    "\n",
    "for key in d:\n",
    "    d[key] = round(d[key], 3)\n",
    "df = pd.DataFrame(d.items(), columns=['Model', 'Macro F1-Score'])\n",
    "print(df.to_latex(float_format=\"%.3f\", index=False, column_format=\"@{}lll@{}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.read_csv(\"final_preds.csv\")\n",
    "final_preds.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "import ast\n",
    "# do literal_eval for each column except label and text\n",
    "for col in final_preds.columns:\n",
    "    if col not in [\"text\"]:\n",
    "        final_preds[col] = final_preds[col].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'bert-base-uncased-probs', 'bert-large-probs',\n",
       "       'roberta-base-probs', 'roberta-large-probs', 'deberta-base-probs',\n",
       "       'deberta-large-probs', 'bert-base-uncased-preds', 'bert-large-preds',\n",
       "       'roberta-base-preds', 'roberta-large-preds', 'deberta-base-preds',\n",
       "       'deberta-large-preds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased 19\n",
      "bert-large-uncased 19\n",
      "roberta-base 19\n",
      "roberta-large 19\n",
      "deberta-base 19\n",
      "deberta-large 19\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.options.display.max_colwidth = 10000\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "macro_f1 = {'bert-base-uncased': 0.16691939082661103, \n",
    "            'bert-large': 0.2695939816828627, \n",
    "            'roberta-base': 0.25323999340304165, \n",
    "            'roberta-large': 0.29354155627190104, \n",
    "            'deberta-base': 0.26874974711093474, \n",
    "            'deberta-large': 0.2988116271661447}\n",
    "\n",
    "macro_f1_models = {'bert-base-uncased': 0.16691939082661103, \n",
    "            'bert-large': 0.2695939816828627, \n",
    "            'roberta-base': 0.25323999340304165, \n",
    "            'roberta-large': 0.29354155627190104, \n",
    "            'deberta-base': 0.26874974711093474, \n",
    "            'deberta-large': 0.2988116271661447}\n",
    "\n",
    "f1_scores_for_classes = {\n",
    "'bert-base-uncased': [0., 0.03146067, 0.21414141, 0.0776699, 0.30545455, 0.28365385, 0.22997172, 0.04545455, 0.13897281, 0.36711479, 0.03375527, 0.37660485, 0., 0., 0.14883721, 0.11604096, 0.28056112, 0.48367953, 0.03809524],\n",
    "'bert-large-uncased' : [0.08450704,0.20591716,0.2539185,0.28220859,0.33764633,0.27488152,0.27322404,0.21276596,0.29580574,0.4,0.37057221,0.42592593,0.14765101,0.04347826,0.24358974,0.22666667,0.33169935,0.54939107, 0.16243655],\n",
    "'roberta-base': [0.07446809, 0.19363057, 0.25505443, 0.2972973, 0.32878493, 0.2647386, 0.2546523, 0.22380952, 0.2962963, 0.35582155, 0.33333333, 0.38797423, 0.07936508, 0., 0.20869565, 0.22516556, 0.31392405, 0.525, 0.19354839],\n",
    "'roberta-large': [0.10328638, 0.23529412, 0.30555556, 0.33121019, 0.36412677, 0.30861244, 0.2875817, 0.23592493, 0.27906977, 0.41947566, 0.43076923, 0.43153527, 0.14893617, 0.03636364, 0.24156306, 0.25112108, 0.34351145, 0.56779661, 0.25555556],\n",
    "'deberta-base': [0.07305936, 0.20790021, 0.23876404, 0.30434783, 0.35170604, 0.2675, 0.27388535, 0.23041475, 0.31042129, 0.3810549, 0.35827664, 0.43902439, 0.16666667, 0., 0.22932331, 0.25652174, 0.32192414, 0.54545455, 0.15],\n",
    "'deberta-large': [0.10714286, 0.23469388, 0.30409357, 0.35294118, 0.36971351, 0.31228473, 0.24236038, 0.26130653, 0.35897436, 0.40338983, 0.43652561, 0.42414861, 0.1732852, 0., 0.27891156, 0.27350427, 0.35959222, 0.54981084, 0.23474178]\n",
    "}\n",
    "\n",
    "for i, j in f1_scores_for_classes.items():\n",
    "    print(i, len(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.331\n"
     ]
    }
   ],
   "source": [
    "final_preds.columns\n",
    "### EQUAL VOTING\n",
    "## sum all -probs columns and divide by the number of columns. Then apply the threshold of 0.5 and compare with the label\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "    acc_col = [ 1 if x/6 > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['equal-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-equal'] = round(f1_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist(), average='macro', zero_division=0),3)\n",
    "f1_scores_for_classes['prob-equal'] = f1_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-equal'] = accuracy_score(final_preds['label'].tolist(), final_preds['equal-voting'].tolist())\n",
    "print(macro_f1['prob-equal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.328\n"
     ]
    }
   ],
   "source": [
    "final_preds.columns\n",
    "## WEIGHTED VOTING. Large models vote double than base models\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            if \"base\" in col:\n",
    "                acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "            if 'large' in col:\n",
    "                acc_col = [acc_col[i] + 2*row[col][i] for i in range(19)]\n",
    "    acc_col = [ 1 if (x/9) > 0.3 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['large-double-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-large-double'] = round(f1_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist(), average='macro', zero_division=0),3)\n",
    "f1_scores_for_classes['prob-large-double'] = f1_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy_score(final_preds['label'].tolist(), final_preds['large-double-voting'].tolist())\n",
    "print(macro_f1['prob-large-double'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.334\n"
     ]
    }
   ],
   "source": [
    "## WEIGTHED VOTING. Models votes with their macro f1-score\n",
    "d = {}\n",
    "\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-probs\" in col:\n",
    "            model = col.replace(\"-probs\", \"\")\n",
    "            acc_col = [acc_col[i] + macro_f1_models[model]*row[col][i] for i in range(19)]\n",
    "    acc_col = [float(x / sum(macro_f1_models.values())) for x in acc_col]\n",
    "    acc_col = [1 if x > 0.2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['f1-score-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['prob-weight-macro-f1'] = round(f1_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['prob-weight-macro-f1'] = f1_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['prob-weight-macro-f1'] = accuracy_score(final_preds['label'].tolist(), final_preds['f1-score-voting'].tolist())\n",
    "print(macro_f1['prob-weight-macro-f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.323\n"
     ]
    }
   ],
   "source": [
    "## MAJORITY VOTING. On preds, the majority wins\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-preds\" in col:\n",
    "            acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "    acc_col = [1 if x > 1 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['majority-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['preds-majority'] = round(f1_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['preds-majority'] = f1_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['preds-majority'] = accuracy_score(final_preds['label'].tolist(), final_preds['majority-voting'].tolist())\n",
    "print(macro_f1['preds-majority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.324\n"
     ]
    }
   ],
   "source": [
    "## MAJORITY VOTING, DOUBLE FOR LARGE MODELS. On preds, the majority wins\n",
    "d = {}\n",
    "for i, row in final_preds.iterrows():\n",
    "    acc_col = [0] * 19\n",
    "    for col in final_preds.columns:\n",
    "        if \"-preds\" in col:\n",
    "            if \"base\" in col:\n",
    "                acc_col = [acc_col[i] + row[col][i] for i in range(19)]\n",
    "            if 'large' in col:\n",
    "                acc_col = [acc_col[i] + 2*row[col][i] for i in range(19)]\n",
    "    acc_col = [1 if x > 2 else 0 for x in acc_col]\n",
    "    d[row.text] = acc_col\n",
    "\n",
    "final_preds['majority-large-double-voting'] = final_preds['text'].map(lambda x: list(d[x]))\n",
    "macro_f1['preds-large-double'] = round(f1_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist(), average='macro', zero_division=0), 3)\n",
    "f1_scores_for_classes['preds-large-double'] = f1_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist(), average=None, zero_division=0).tolist()\n",
    "accuracy['preds-large-double'] = accuracy_score(final_preds['label'].tolist(), final_preds['majority-large-double-voting'].tolist())\n",
    "print(macro_f1['preds-large-double'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Model & Macro F1-Score \\\\\n",
      "\\midrule\n",
      "bert-base-uncased & 0.167 \\\\\n",
      "bert-large & 0.270 \\\\\n",
      "roberta-base & 0.253 \\\\\n",
      "roberta-large & 0.294 \\\\\n",
      "deberta-base & 0.269 \\\\\n",
      "deberta-large & 0.299 \\\\\n",
      "prob-equal & 0.331 \\\\\n",
      "prob-large-double & 0.328 \\\\\n",
      "prob-weight-macro-f1 & 0.334 \\\\\n",
      "preds-majority & 0.323 \\\\\n",
      "preds-large-double & 0.324 \\\\\n",
      "Arthur Schopenhauer $\\star$ $\\dagger$ & {\\cellcolor{gray!25}} 0.441 \\\\\n",
      "Philo of Alexandria & 0.200 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_54539/513005734.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  macro_f1['Arthur Schopenhauer $\\star$ $\\dagger$'] = 0.4405\n",
      "/var/folders/c9/g15qwq357gg9q2c0brnkt5mh0000gn/T/ipykernel_54539/513005734.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  .to_latex(convert_css=True, hrules=True, caption=\"Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version\",position_float='centering')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# macro_f1['Hierocles of Alexandria $\\dagger$'] = 0.39 \n",
    "macro_f1['Arthur Schopenhauer $\\star$ $\\dagger$'] = 0.4405 \n",
    "macro_f1['Philo of Alexandria'] = 0.20\n",
    "df = pd.DataFrame(macro_f1.items(), columns=['Model', 'Macro F1-Score'])\n",
    "\n",
    "print ( df.style\\\n",
    ".highlight_max(color='gray!25', subset=['Macro F1-Score'] ).hide(level=0, axis=0) \\\n",
    ".format(\"{:.3f}\", subset=['Macro F1-Score']) \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"Macro F1-Score Scores for the trained models. $\\star$ means the model is an ensemble, and $\\dagger$ means it used the multilingual dataset version\",position_float='centering')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance']\n",
    "\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for f1_scores_for_classes using the existing dict and mapping the ids to labels\n",
    "f1_scores_for_classes_df = pd.DataFrame(f1_scores_for_classes, index=[id2label[i] for i in range(19)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{F1-Scores for each human value}\n",
      "\\begin{tabular}{lrrrrrrrrrrr}\n",
      "\\toprule\n",
      " & \\rotatebox{90}{bert-base-uncased} & \\rotatebox{90}{bert-large-uncased} & \\rotatebox{90}{roberta-base} & \\rotatebox{90}{roberta-large} & \\rotatebox{90}{deberta-base} & \\rotatebox{90}{deberta-large} & \\rotatebox{90}{prob-equal} & \\rotatebox{90}{prob-large-double} & \\rotatebox{90}{prob-weight-macro-f1} & \\rotatebox{90}{preds-majority} & \\rotatebox{90}{preds-large-double} \\\\\n",
      "\\midrule\n",
      "Self-direction: thought & 00 & 08 & 07 & 10 & 07 & 11 & 13 & 14 & {\\cellcolor{gray!25}} 14 & 13 & 13 \\\\\n",
      "Self-direction: action & 03 & 21 & 19 & 24 & 21 & 23 & 24 & {\\cellcolor{gray!25}} 26 & 25 & 26 & 26 \\\\\n",
      "Stimulation & 21 & 25 & 26 & 31 & 24 & 30 & {\\cellcolor{gray!25}} 35 & 35 & 34 & 34 & 35 \\\\\n",
      "Hedonism & 08 & 28 & 30 & 33 & 30 & 35 & {\\cellcolor{gray!25}} 38 & 36 & 37 & 37 & 37 \\\\\n",
      "Achievement & 31 & 34 & 33 & 36 & 35 & 37 & 40 & 40 & {\\cellcolor{gray!25}} 41 & 40 & 40 \\\\\n",
      "Power: dominance & 28 & 27 & 26 & 31 & 27 & 31 & {\\cellcolor{gray!25}} 36 & 35 & 36 & 35 & 35 \\\\\n",
      "Power: resources & 23 & 27 & 25 & 29 & 27 & 24 & 33 & 32 & {\\cellcolor{gray!25}} 34 & 32 & 32 \\\\\n",
      "Face & 05 & 21 & 22 & 24 & 23 & 26 & 30 & 30 & {\\cellcolor{gray!25}} 32 & 29 & 30 \\\\\n",
      "Security: personal & 14 & 30 & 30 & 28 & 31 & 36 & 36 & 36 & {\\cellcolor{gray!25}} 36 & 35 & 36 \\\\\n",
      "Security: societal & 37 & 40 & 36 & 42 & 38 & 40 & 45 & 45 & 45 & 45 & {\\cellcolor{gray!25}} 45 \\\\\n",
      "Tradition & 03 & 37 & 33 & 43 & 36 & 44 & 44 & 44 & {\\cellcolor{gray!25}} 45 & 44 & 44 \\\\\n",
      "Conformity: rules & 38 & 43 & 39 & 43 & 44 & 42 & 49 & 49 & 49 & {\\cellcolor{gray!25}} 49 & 48 \\\\\n",
      "Conformity: interpersonal & 00 & 15 & 08 & 15 & 17 & 17 & 20 & {\\cellcolor{gray!25}} 20 & 20 & 18 & 18 \\\\\n",
      "Humility & 00 & 04 & 00 & 04 & 00 & 00 & {\\cellcolor{gray!25}} 04 & 00 & 04 & 00 & 00 \\\\\n",
      "Benevolence: caring & 15 & 24 & 21 & 24 & 23 & 28 & 31 & {\\cellcolor{gray!25}} 31 & 31 & 29 & 28 \\\\\n",
      "Benevolence: dependability & 12 & 23 & 23 & 25 & 26 & 27 & 29 & 29 & 29 & 29 & {\\cellcolor{gray!25}} 30 \\\\\n",
      "Universalism: concern & 28 & 33 & 31 & 34 & 32 & 36 & 37 & {\\cellcolor{gray!25}} 38 & 37 & 38 & 38 \\\\\n",
      "Universalism: nature & 48 & 55 & 53 & 57 & 55 & 55 & 58 & {\\cellcolor{gray!25}} 59 & 58 & 59 & 59 \\\\\n",
      "Universalism: tolerance & 04 & 16 & 19 & 26 & 15 & 23 & 25 & 26 & {\\cellcolor{gray!25}} 28 & 22 & 22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_format = lambda x: f\"{x:.2f}\".split('.')[1]\n",
    "\n",
    "latex_table = f1_scores_for_classes_df.style \\\n",
    ".highlight_max(color='gray!25', axis=1) \\\n",
    ".format(float_format, subset=f1_scores_for_classes_df.columns)   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"F1-Scores for each human value\",position_float='centering')\n",
    "\n",
    "for column in f1_scores_for_classes_df.columns:\n",
    "    latex_table = latex_table.replace(column, f'\\\\rotatebox{{90}}{{{column}}}')\n",
    "\n",
    "print(latex_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hierocles of Alexandria</th>\n",
       "      <th>Arthur Schopenhauer</th>\n",
       "      <th>Philo of Alexandria</th>\n",
       "      <th>prob-weight-macro-f1</th>\n",
       "      <th>preds-large-double</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.022140</td>\n",
       "      <td>13.197970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self-direction: action</th>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.872232</td>\n",
       "      <td>25.677267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stimulation</th>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.266517</td>\n",
       "      <td>34.626039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedonism</th>\n",
       "      <td>37.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.073171</td>\n",
       "      <td>37.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Achievement</th>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40.581162</td>\n",
       "      <td>39.930556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: dominance</th>\n",
       "      <td>42.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.676626</td>\n",
       "      <td>34.755463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: resources</th>\n",
       "      <td>49.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.652008</td>\n",
       "      <td>32.068164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Face</th>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.939163</td>\n",
       "      <td>29.879518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: personal</th>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.097561</td>\n",
       "      <td>35.814889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: societal</th>\n",
       "      <td>49.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.910891</td>\n",
       "      <td>45.480226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tradition</th>\n",
       "      <td>46.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>44.356436</td>\n",
       "      <td>44.393593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: rules</th>\n",
       "      <td>51.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>49.153468</td>\n",
       "      <td>48.431619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.151134</td>\n",
       "      <td>17.880795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humility</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>30.569948</td>\n",
       "      <td>28.346457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.470199</td>\n",
       "      <td>29.690722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: concern</th>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.159920</td>\n",
       "      <td>37.660256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: nature</th>\n",
       "      <td>63.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.006719</td>\n",
       "      <td>58.650307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.536232</td>\n",
       "      <td>22.325581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Hierocles of Alexandria  Arthur Schopenhauer  Philo of Alexandria  prob-weight-macro-f1  preds-large-double\n",
       "Self-direction: thought                        15.0                 12.0                  8.0             14.022140           13.197970\n",
       "Self-direction: action                         27.0                 24.0                 22.0             24.872232           25.677267\n",
       "Stimulation                                    30.0                 33.0                 27.0             34.266517           34.626039\n",
       "Hedonism                                       37.0                 35.0                 31.0             37.073171           37.037037\n",
       "Achievement                                    45.0                 40.0                 35.0             40.581162           39.930556\n",
       "Power: dominance                               42.0                 37.0                 31.0             35.676626           34.755463\n",
       "Power: resources                               49.0                 47.0                 34.0             33.652008           32.068164\n",
       "Face                                           31.0                 24.0                 17.0             31.939163           29.879518\n",
       "Security: personal                             42.0                 38.0                 33.0             36.097561           35.814889\n",
       "Security: societal                             49.0                 46.0                 40.0             44.910891           45.480226\n",
       "Tradition                                      46.0                 49.0                 47.0             44.356436           44.393593\n",
       "Conformity: rules                              51.0                 50.0                 42.0             49.153468           48.431619\n",
       "Conformity: interpersonal                      24.0                 19.0                  9.0             20.151134           17.880795\n",
       "Humility                                        0.0                  0.0                  0.0              4.166667            0.000000\n",
       "Benevolence: caring                            34.0                 32.0                 21.0             30.569948           28.346457\n",
       "Benevolence: dependability                     33.0                 31.0                 28.0             29.470199           29.690722\n",
       "Universalism: concern                          47.0                 46.0                 40.0             37.159920           37.660256\n",
       "Universalism: nature                           63.0                 60.0                 57.0             58.006719           58.650307\n",
       "Universalism: tolerance                        27.0                 27.0                 21.0             27.536232           22.325581"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores_for_classes_2 = {\n",
    "'Hierocles of Alexandria': [ 15., 27., 30., 37., 45., 42., 49., 31., 42., 49., 46., 51., 24., 00., 34., 33., 47., 63., 27],\n",
    "'Arthur Schopenhauer':     [12., 24., 33., 35., 40., 37., 47., 24., 38., 46., 49., 50., 19., 00., 32., 31., 46., 60., 27],\n",
    "'Philo of Alexandria':     [ 8., 22., 27., 31., 35., 31., 34., 17., 33., 40., 47., 42., 9., 00., 21., 28., 40., 57., 21],\n",
    "'prob-weight-macro-f1':    [x*100 for x in f1_scores_for_classes['prob-weight-macro-f1']],\n",
    "'preds-large-double':      [x*100 for x in f1_scores_for_classes['preds-large-double']]\n",
    "}\n",
    "\n",
    "f1_general = pd.DataFrame(f1_scores_for_classes_2, index=[id2label[i] for i in range(19)])\n",
    "f1_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{F1-Scores for each human value}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      " & \\rotatebox{90}{Hierocles of Alexandria} & \\rotatebox{90}{Arthur Schopenhauer} & \\rotatebox{90}{Philo of Alexandria} & \\rotatebox{90}{prob-weight-macro-f1} & \\rotatebox{90}{preds-large-double} \\\\\n",
      "\\midrule\n",
      "Self-direction: thought & {\\cellcolor{gray!25}} 15 & 12 & 8 & 14 & 13 \\\\\n",
      "Self-direction: action & {\\cellcolor{gray!25}} 27 & 24 & 22 & 24 & 25 \\\\\n",
      "Stimulation & 30 & 33 & 27 & 34 & {\\cellcolor{gray!25}} 34 \\\\\n",
      "Hedonism & 37 & 35 & 31 & {\\cellcolor{gray!25}} 37 & 37 \\\\\n",
      "Achievement & {\\cellcolor{gray!25}} 45 & 40 & 35 & 40 & 39 \\\\\n",
      "Power: dominance & {\\cellcolor{gray!25}} 42 & 37 & 31 & 35 & 34 \\\\\n",
      "Power: resources & {\\cellcolor{gray!25}} 49 & 47 & 34 & 33 & 32 \\\\\n",
      "Face & 31 & 24 & 17 & {\\cellcolor{gray!25}} 31 & 29 \\\\\n",
      "Security: personal & {\\cellcolor{gray!25}} 42 & 38 & 33 & 36 & 35 \\\\\n",
      "Security: societal & {\\cellcolor{gray!25}} 49 & 46 & 40 & 44 & 45 \\\\\n",
      "Tradition & 46 & {\\cellcolor{gray!25}} 49 & 47 & 44 & 44 \\\\\n",
      "Conformity: rules & {\\cellcolor{gray!25}} 51 & 50 & 42 & 49 & 48 \\\\\n",
      "Conformity: interpersonal & {\\cellcolor{gray!25}} 24 & 19 & 9 & 20 & 17 \\\\\n",
      "Humility & 0 & 0 & 0 & {\\cellcolor{gray!25}} 4 & 0 \\\\\n",
      "Benevolence: caring & {\\cellcolor{gray!25}} 34 & 32 & 21 & 30 & 28 \\\\\n",
      "Benevolence: dependability & {\\cellcolor{gray!25}} 33 & 31 & 28 & 29 & 29 \\\\\n",
      "Universalism: concern & {\\cellcolor{gray!25}} 47 & 46 & 40 & 37 & 37 \\\\\n",
      "Universalism: nature & {\\cellcolor{gray!25}} 63 & 60 & 57 & 58 & 58 \\\\\n",
      "Universalism: tolerance & 27 & 27 & 21 & {\\cellcolor{gray!25}} 27 & 22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "float_format = lambda x: f\"{x:.02f}\".split('.')[0]\n",
    "\n",
    "latex_table = f1_general.style \\\n",
    ".highlight_max(color='gray!25', axis=1) \\\n",
    ".format(float_format, subset=f1_general.columns)   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"F1-Scores for each human value\",position_float='centering')\n",
    "\n",
    "for column in f1_general.columns:\n",
    "    latex_table = latex_table.replace(column, f'\\\\rotatebox{{90}}{{{column}}}')\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Accuracy Scores for the trained models}\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Model & Accuracy \\\\\n",
      "\\midrule\n",
      "roberta-large & {\\cellcolor{gray!25}} 0.522 \\\\\n",
      "deberta-large & 0.520 \\\\\n",
      "bert-base-uncased & 0.513 \\\\\n",
      "preds-large-double & 0.503 \\\\\n",
      "roberta-base & 0.497 \\\\\n",
      "preds-majority & 0.493 \\\\\n",
      "deberta-base & 0.489 \\\\\n",
      "bert-large-uncased & 0.487 \\\\\n",
      "prob-equal & 0.452 \\\\\n",
      "prob-weight-macro-f1 & 0.451 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = dict(sorted(accuracy.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "accuracy_df = pd.DataFrame(accuracy.items(), columns=['Model', 'Accuracy']).style \\\n",
    ".highlight_max(color='gray!25', subset=['Accuracy'] ).hide(level=0, axis=0) \\\n",
    ".format(\"{:.3f}\", subset=['Accuracy'])   \\\n",
    ".to_latex(convert_css=True, hrules=True, caption=\"Accuracy Scores for the trained models\",position_float='centering')\n",
    "print(accuracy_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
